{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "NLP Workshop: Text cleaning, topic modeling and classification",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rbkhb/NLP_IMC/blob/master/NLP_Workshop_Text_cleaning%2C_topic_modeling_and_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ammwognrH3vj",
        "colab_type": "text"
      },
      "source": [
        "-------\n",
        "## AU Interacting Minds Centre, NLP Workshop - November 7th \n",
        "\n",
        "**Stance Detection & Topic Modelling of Social Media Users' Content**  \n",
        "\n",
        "*Rebekah Baglini, Luca Nannini, and Arnault-Quentin Vermillet*\n",
        "\n",
        "\n",
        "\n",
        "![alt text](https://docs.google.com/drawings/d/e/2PACX-1vTRkUtZJSFMxPWXaidljOqwNDnFTLb4E3GWB6AsqVfcWdYKsI4y9f8EaCz2yozWRe4I8vnvePngC-TM/pub?w=1393&h=614)\n",
        "\n",
        "\n",
        "### Program\n",
        "\n",
        "1. Data Preprocessing \n",
        " - Load Dataset \n",
        " - Tokenization/Stopword Removal\n",
        " - Clean Tweets Strings with Regular Expressions\n",
        " - Lemmatization/Stemming\n",
        "\n",
        "2. Topic modeling\n",
        " - Create, Run, and Train the HDP model via Gensim \n",
        " - Visualize topics through an interactive graphs - pyLDAvis \n",
        " - Visualize cosine metrics of topics as a heatmap  \n",
        " - HDP and LDA via Gensim Models\n",
        "\n",
        " 3. Supervised text classification with BERT\n",
        "\n",
        "### Datasets\n",
        "\n",
        "Vacc_tweets_raw_n5000.csv\n",
        "  * Random sample of 5000 (out of > 1 million) tweets from 2019 containing string 'vaccin'\n",
        "  * Collected using [GetOldTweets3 scraper](https://github.com/Jefferson-Henrique/GetOldTweets-python)\n",
        "\n",
        "#### Additional sets in Data folder \n",
        "5-topic_stance_tweets_training_n2814.csv\n",
        "  * Training set from [SemEval2016 Task 6](http://alt.qcri.org/semeval2016/task6/) for stance detection task\n",
        "  * Labels: FAVOR, AGAINST, UNKNOWN\n",
        "  * Topics: \n",
        "    - Atheism\n",
        "    - Climate change is a concern\n",
        "    - Feminist movement\n",
        "    - Hillary Clinton\n",
        "    - Legalization of abortion\n",
        "\n",
        "Vacc_articles_w_stance_n3303.csv\n",
        "  * From [Vaccine sentiment project](https://github.com/gloriakang/vax-sentiment), contains 3303 sentences extracted from online articles labelled pro (n=24), neg (n=22), and neu(tral) (n=7).\n",
        "\n",
        "Vacc_tweets_w_stance_n1131.csv\n",
        "* 1131 tweets containing string 'vaccin' labeled for stance\n",
        "* Labels = pro, anti, unknown\n",
        "\n",
        "**Or upload your own dataset!**\n",
        "* Using the upload widget in the Colab file. \n",
        "\n",
        "### **Code files**\n",
        "* During the tutorial, we will be working from a Google Colab notebook. This means you will not have to install or load anything locally. \n",
        "* If you'd like to run locally, we've included list of dependencies in Requirements.txt\n",
        "\n",
        "\n",
        "\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRxT7D_w48mf",
        "colab_type": "text"
      },
      "source": [
        "### Libraries & Packages required\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYifLAuFH3vm",
        "colab_type": "code",
        "outputId": "8d4a5286-e531-4609-f49b-25b2a91fdfdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np \n",
        "import re\n",
        "import os \n",
        "import string\n",
        "from string import punctuation\n",
        "import _collections\n",
        "from _collections import defaultdict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore',category=DeprecationWarning)\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import gensim\n",
        "from gensim import corpora, models, similarities\n",
        "from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel\n",
        "from gensim.corpora import Dictionary"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSg1W4nkhHLf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import sys\n",
        "#!{sys.executable} -m pip install spacy\n",
        "#!{sys.executable} -m spacy download en"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ug1FWTFM19Wa",
        "colab_type": "code",
        "outputId": "cc5e650e-0274-4358-a1bb-f8215743e5f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "!pip install pyLDAvis \n",
        "import pyLDAvis.gensim"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyLDAvis in /usr/local/lib/python3.6/dist-packages (2.1.2)\n",
            "Requirement already satisfied: funcy in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.13)\n",
            "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.17.3)\n",
            "Requirement already satisfied: jinja2>=2.7.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (2.10.3)\n",
            "Requirement already satisfied: scipy>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.3.1)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (2.7.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.16.0)\n",
            "Requirement already satisfied: joblib>=0.8.4 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.14.0)\n",
            "Requirement already satisfied: wheel>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.33.6)\n",
            "Requirement already satisfied: pandas>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.25.3)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (3.6.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.7.2->pyLDAvis) (1.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyLDAvis) (2.6.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyLDAvis) (2018.9)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (19.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.12.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.3.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.8.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (0.7.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (7.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (41.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTVd4V1Dg3vh",
        "colab_type": "text"
      },
      "source": [
        "## 1. Data Preprocessing \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3L4_9Fz3SXQ",
        "colab_type": "text"
      },
      "source": [
        "### Load Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Zjb3Yv37CWHH"
      },
      "source": [
        "**Part 1 assumes GOT3 output formatted text.**\n",
        "\n",
        "Download Raw_vacc_tweets_n5000.csv from [Github](https://github.com/rbkhb/NLP_IMC), load from your local drive.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdbxf_SUH3v-",
        "colab_type": "code",
        "outputId": "850516dc-9e67-4571-f842-beffcffb3cee",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7Ci8vIE1heCBhbW91bnQgb2YgdGltZSB0byBibG9jayB3YWl0aW5nIGZvciB0aGUgdXNlci4KY29uc3QgRklMRV9DSEFOR0VfVElNRU9VVF9NUyA9IDMwICogMTAwMDsKCmZ1bmN0aW9uIF91cGxvYWRGaWxlcyhpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IHN0ZXBzID0gdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKTsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIC8vIENhY2hlIHN0ZXBzIG9uIHRoZSBvdXRwdXRFbGVtZW50IHRvIG1ha2UgaXQgYXZhaWxhYmxlIGZvciB0aGUgbmV4dCBjYWxsCiAgLy8gdG8gdXBsb2FkRmlsZXNDb250aW51ZSBmcm9tIFB5dGhvbi4KICBvdXRwdXRFbGVtZW50LnN0ZXBzID0gc3RlcHM7CgogIHJldHVybiBfdXBsb2FkRmlsZXNDb250aW51ZShvdXRwdXRJZCk7Cn0KCi8vIFRoaXMgaXMgcm91Z2hseSBhbiBhc3luYyBnZW5lcmF0b3IgKG5vdCBzdXBwb3J0ZWQgaW4gdGhlIGJyb3dzZXIgeWV0KSwKLy8gd2hlcmUgdGhlcmUgYXJlIG11bHRpcGxlIGFzeW5jaHJvbm91cyBzdGVwcyBhbmQgdGhlIFB5dGhvbiBzaWRlIGlzIGdvaW5nCi8vIHRvIHBvbGwgZm9yIGNvbXBsZXRpb24gb2YgZWFjaCBzdGVwLgovLyBUaGlzIHVzZXMgYSBQcm9taXNlIHRvIGJsb2NrIHRoZSBweXRob24gc2lkZSBvbiBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcCwKLy8gdGhlbiBwYXNzZXMgdGhlIHJlc3VsdCBvZiB0aGUgcHJldmlvdXMgc3RlcCBhcyB0aGUgaW5wdXQgdG8gdGhlIG5leHQgc3RlcC4KZnVuY3Rpb24gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpIHsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIGNvbnN0IHN0ZXBzID0gb3V0cHV0RWxlbWVudC5zdGVwczsKCiAgY29uc3QgbmV4dCA9IHN0ZXBzLm5leHQob3V0cHV0RWxlbWVudC5sYXN0UHJvbWlzZVZhbHVlKTsKICByZXR1cm4gUHJvbWlzZS5yZXNvbHZlKG5leHQudmFsdWUucHJvbWlzZSkudGhlbigodmFsdWUpID0+IHsKICAgIC8vIENhY2hlIHRoZSBsYXN0IHByb21pc2UgdmFsdWUgdG8gbWFrZSBpdCBhdmFpbGFibGUgdG8gdGhlIG5leHQKICAgIC8vIHN0ZXAgb2YgdGhlIGdlbmVyYXRvci4KICAgIG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSA9IHZhbHVlOwogICAgcmV0dXJuIG5leHQudmFsdWUucmVzcG9uc2U7CiAgfSk7Cn0KCi8qKgogKiBHZW5lcmF0b3IgZnVuY3Rpb24gd2hpY2ggaXMgY2FsbGVkIGJldHdlZW4gZWFjaCBhc3luYyBzdGVwIG9mIHRoZSB1cGxvYWQKICogcHJvY2Vzcy4KICogQHBhcmFtIHtzdHJpbmd9IGlucHV0SWQgRWxlbWVudCBJRCBvZiB0aGUgaW5wdXQgZmlsZSBwaWNrZXIgZWxlbWVudC4KICogQHBhcmFtIHtzdHJpbmd9IG91dHB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIG91dHB1dCBkaXNwbGF5LgogKiBAcmV0dXJuIHshSXRlcmFibGU8IU9iamVjdD59IEl0ZXJhYmxlIG9mIG5leHQgc3RlcHMuCiAqLwpmdW5jdGlvbiogdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKSB7CiAgY29uc3QgaW5wdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQoaW5wdXRJZCk7CiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gZmFsc2U7CgogIGNvbnN0IG91dHB1dEVsZW1lbnQgPSBkb2N1bWVudC5nZXRFbGVtZW50QnlJZChvdXRwdXRJZCk7CiAgb3V0cHV0RWxlbWVudC5pbm5lckhUTUwgPSAnJzsKCiAgY29uc3QgcGlja2VkUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBpbnB1dEVsZW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignY2hhbmdlJywgKGUpID0+IHsKICAgICAgcmVzb2x2ZShlLnRhcmdldC5maWxlcyk7CiAgICB9KTsKICB9KTsKCiAgY29uc3QgY2FuY2VsID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnYnV0dG9uJyk7CiAgaW5wdXRFbGVtZW50LnBhcmVudEVsZW1lbnQuYXBwZW5kQ2hpbGQoY2FuY2VsKTsKICBjYW5jZWwudGV4dENvbnRlbnQgPSAnQ2FuY2VsIHVwbG9hZCc7CiAgY29uc3QgY2FuY2VsUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBjYW5jZWwub25jbGljayA9ICgpID0+IHsKICAgICAgcmVzb2x2ZShudWxsKTsKICAgIH07CiAgfSk7CgogIC8vIENhbmNlbCB1cGxvYWQgaWYgdXNlciBoYXNuJ3QgcGlja2VkIGFueXRoaW5nIGluIHRpbWVvdXQuCiAgY29uc3QgdGltZW91dFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgc2V0VGltZW91dCgoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9LCBGSUxFX0NIQU5HRV9USU1FT1VUX01TKTsKICB9KTsKCiAgLy8gV2FpdCBmb3IgdGhlIHVzZXIgdG8gcGljayB0aGUgZmlsZXMuCiAgY29uc3QgZmlsZXMgPSB5aWVsZCB7CiAgICBwcm9taXNlOiBQcm9taXNlLnJhY2UoW3BpY2tlZFByb21pc2UsIHRpbWVvdXRQcm9taXNlLCBjYW5jZWxQcm9taXNlXSksCiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdzdGFydGluZycsCiAgICB9CiAgfTsKCiAgaWYgKCFmaWxlcykgewogICAgcmV0dXJuIHsKICAgICAgcmVzcG9uc2U6IHsKICAgICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICAgIH0KICAgIH07CiAgfQoKICBjYW5jZWwucmVtb3ZlKCk7CgogIC8vIERpc2FibGUgdGhlIGlucHV0IGVsZW1lbnQgc2luY2UgZnVydGhlciBwaWNrcyBhcmUgbm90IGFsbG93ZWQuCiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gdHJ1ZTsKCiAgZm9yIChjb25zdCBmaWxlIG9mIGZpbGVzKSB7CiAgICBjb25zdCBsaSA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2xpJyk7CiAgICBsaS5hcHBlbmQoc3BhbihmaWxlLm5hbWUsIHtmb250V2VpZ2h0OiAnYm9sZCd9KSk7CiAgICBsaS5hcHBlbmQoc3BhbigKICAgICAgICBgKCR7ZmlsZS50eXBlIHx8ICduL2EnfSkgLSAke2ZpbGUuc2l6ZX0gYnl0ZXMsIGAgKwogICAgICAgIGBsYXN0IG1vZGlmaWVkOiAkewogICAgICAgICAgICBmaWxlLmxhc3RNb2RpZmllZERhdGUgPyBmaWxlLmxhc3RNb2RpZmllZERhdGUudG9Mb2NhbGVEYXRlU3RyaW5nKCkgOgogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAnbi9hJ30gLSBgKSk7CiAgICBjb25zdCBwZXJjZW50ID0gc3BhbignMCUgZG9uZScpOwogICAgbGkuYXBwZW5kQ2hpbGQocGVyY2VudCk7CgogICAgb3V0cHV0RWxlbWVudC5hcHBlbmRDaGlsZChsaSk7CgogICAgY29uc3QgZmlsZURhdGFQcm9taXNlID0gbmV3IFByb21pc2UoKHJlc29sdmUpID0+IHsKICAgICAgY29uc3QgcmVhZGVyID0gbmV3IEZpbGVSZWFkZXIoKTsKICAgICAgcmVhZGVyLm9ubG9hZCA9IChlKSA9PiB7CiAgICAgICAgcmVzb2x2ZShlLnRhcmdldC5yZXN1bHQpOwogICAgICB9OwogICAgICByZWFkZXIucmVhZEFzQXJyYXlCdWZmZXIoZmlsZSk7CiAgICB9KTsKICAgIC8vIFdhaXQgZm9yIHRoZSBkYXRhIHRvIGJlIHJlYWR5LgogICAgbGV0IGZpbGVEYXRhID0geWllbGQgewogICAgICBwcm9taXNlOiBmaWxlRGF0YVByb21pc2UsCiAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgYWN0aW9uOiAnY29udGludWUnLAogICAgICB9CiAgICB9OwoKICAgIC8vIFVzZSBhIGNodW5rZWQgc2VuZGluZyB0byBhdm9pZCBtZXNzYWdlIHNpemUgbGltaXRzLiBTZWUgYi82MjExNTY2MC4KICAgIGxldCBwb3NpdGlvbiA9IDA7CiAgICB3aGlsZSAocG9zaXRpb24gPCBmaWxlRGF0YS5ieXRlTGVuZ3RoKSB7CiAgICAgIGNvbnN0IGxlbmd0aCA9IE1hdGgubWluKGZpbGVEYXRhLmJ5dGVMZW5ndGggLSBwb3NpdGlvbiwgTUFYX1BBWUxPQURfU0laRSk7CiAgICAgIGNvbnN0IGNodW5rID0gbmV3IFVpbnQ4QXJyYXkoZmlsZURhdGEsIHBvc2l0aW9uLCBsZW5ndGgpOwogICAgICBwb3NpdGlvbiArPSBsZW5ndGg7CgogICAgICBjb25zdCBiYXNlNjQgPSBidG9hKFN0cmluZy5mcm9tQ2hhckNvZGUuYXBwbHkobnVsbCwgY2h1bmspKTsKICAgICAgeWllbGQgewogICAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgICBhY3Rpb246ICdhcHBlbmQnLAogICAgICAgICAgZmlsZTogZmlsZS5uYW1lLAogICAgICAgICAgZGF0YTogYmFzZTY0LAogICAgICAgIH0sCiAgICAgIH07CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPQogICAgICAgICAgYCR7TWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCl9JSBkb25lYDsKICAgIH0KICB9CgogIC8vIEFsbCBkb25lLgogIHlpZWxkIHsKICAgIHJlc3BvbnNlOiB7CiAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgIH0KICB9Owp9CgpzY29wZS5nb29nbGUgPSBzY29wZS5nb29nbGUgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYiA9IHNjb3BlLmdvb2dsZS5jb2xhYiB8fCB7fTsKc2NvcGUuZ29vZ2xlLmNvbGFiLl9maWxlcyA9IHsKICBfdXBsb2FkRmlsZXMsCiAgX3VwbG9hZEZpbGVzQ29udGludWUsCn07Cn0pKHNlbGYpOwo=",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 37
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-32be86f9-2792-42f8-9b59-777b8f95c772\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-32be86f9-2792-42f8-9b59-777b8f95c772\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Jioqi21ZH3wC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Check that yout filename matches that assigned by the Upload Widget above. \n",
        "df = pd.read_csv('Raw_vacc_tweets_n5000 (3).csv',encoding=\"utf8\")\n",
        "df.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYTD9mnoH3wG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Let's peek at our dataframe\n",
        "df.head(20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uum_Yb7LSFiO",
        "colab_type": "text"
      },
      "source": [
        "## Cleaning up the tweets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbVlot_4U9eR",
        "colab_type": "text"
      },
      "source": [
        "### Extracting URLs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7P3F5kB_Daz",
        "colab_type": "text"
      },
      "source": [
        "External links are often informative, but URLS will add unwanted noise to our language models. \n",
        "\n",
        "Therefore, we strip out hyperlinks found in tweets and copy them to a new column called 'URLS'. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "-PSJ6dMmH3wR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We first define a function that finds URLs using regex\n",
        "def find_URLs(tweets):\n",
        "    return re.findall(r\"((?:https?:\\/\\/(?:www\\.)?|(?:pic\\.|www\\.)(?:\\S*\\.))(?:\\S*))\", tweets)\n",
        "\n",
        "#We apply the function to our text column of our data frame\n",
        "df['URLs'] = df.text.apply(find_URLs) \n",
        "df['URLs'].head(20)\n",
        "\n",
        "#Then we can get rid of them inside of the tweet\n",
        "df['clean_text'] = [re.sub(r\"((?:https?:\\/\\/(?:www\\.)?|(?:pic\\.|www\\.)(?:\\S*\\.))(?:\\S*))\",'', x) for x in df['text']]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsDW4xRFQhlr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Let's look at some examples to see what happened\n",
        "print(df['text'][1])\n",
        "print(df['clean_text'][1])\n",
        "print(df['URLs'][1])\n",
        "print(\" \\n\")\n",
        "print(df['text'][6])\n",
        "print(df['clean_text'][6])\n",
        "print(df['URLs'][6])\n",
        "print(\" \\n\")\n",
        "\n",
        "print(df['text'][19])\n",
        "print(df['clean_text'][19])\n",
        "print(df['URLs'][19])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJ-ElXus9_tn",
        "colab_type": "text"
      },
      "source": [
        "### Clean Tweets Strings with Regular Expression "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qfuiaf-UcVR9",
        "colab_type": "text"
      },
      "source": [
        "We are now standardizing the informal language of tweets, stripping out user mentions, hashtags, punctuation, etc. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1mPpL89VZF4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We start by defining lists of words to remove\n",
        "my_stopwords = nltk.corpus.stopwords.words('english') #uninformative common words\n",
        "my_punctuation = r'!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~•…' #punctuation\n",
        "#We specify the stemmer or lemmatizer we want to use\n",
        "word_rooter = nltk.stem.snowball.PorterStemmer(ignore_stopwords=False).stem\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "#And we define a cleaning master function to do the heavy lifting\n",
        "def clean_tweet(tweet, bigrams=False, lemma=False):\n",
        "    tweet = tweet.lower() # lower case\n",
        "    tweet = re.sub(r'[^\\w\\s]', ' ', tweet) # strip punctuation\n",
        "    tweet = re.sub(r'\\s+', ' ', tweet) #remove double spacing\n",
        "    tweet = re.sub(r'([0-9]+)', '', tweet) # remove numbers\n",
        "    tweet = re.sub(r'([\\U00002024-\\U00002026]+)', '', tweet) #removing html tag (\"...\" where a link used to be)\n",
        "    tweet_token_list = [word for word in tweet.split(' ')\n",
        "                            if word not in my_stopwords] # remove stopwords\n",
        "\n",
        "    if lemma == True:\n",
        "      tweet_token_list = [wordnet_lemmatizer.lemmatize(word) if '#' not in word else word\n",
        "                        for word in tweet_token_list] # apply lemmatizer\n",
        "    else:   # or                 \n",
        "      tweet_token_list = [word_rooter(word) if '#' not in word else word\n",
        "                        for word in tweet_token_list] # apply word rooter\n",
        "    if bigrams:\n",
        "        tweet_token_list = tweet_token_list+[tweet_token_list[i]+'_'+tweet_token_list[i+1]\n",
        "                                            for i in range(len(tweet_token_list)-1)]\n",
        "    tweet = ' '.join(tweet_token_list)\n",
        "    return tweet\n",
        "\n",
        "#Finally we apply the function to clean tweets (here we use lemmas)\n",
        "df['clean_text'] = df.clean_text.apply(clean_tweet, lemma=True)\n",
        "\n",
        "df.head(20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5QB4eZMBTdLs",
        "colab": {}
      },
      "source": [
        "#Again, Let's take the same examples and look at what happened\n",
        "print(df['text'][1])\n",
        "print(df['clean_text'][1])\n",
        "print(df['URLs'][1])\n",
        "print(\" \\n\")\n",
        "\n",
        "print(df['text'][8])\n",
        "print(df['clean_text'][8])\n",
        "print(df['URLs'][8])\n",
        "print(\" \\n\")\n",
        "\n",
        "print(df['text'][19])\n",
        "print(df['clean_text'][19])\n",
        "print(df['URLs'][19])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rjsmJ4U-RgC",
        "colab_type": "text"
      },
      "source": [
        "#### We can now tokenize each tweet in preparation for topic modeling\n",
        "\n",
        "And before we do so, we might take an extra step.  \n",
        "To be efficient, we probably should have done this before, but for the sake of clarity, let's do it now.  \n",
        "  \n",
        "Depending on your dataset, there might be specific vocabulary that is very significant but not informative (like \"vaccine\" in our case). Therefore, we might want to desigh a custom list of stop words that we will remove from the tweets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TLfPux0mddk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We define our list so that we take out those words when tokenising\n",
        "custom_stop_words = ['vaccine', 'vaccinate']\n",
        "\n",
        "#We create a new column with tokens\n",
        "df['token_text'] = [\n",
        "    [word for word in tweet.split()  if word not in custom_stop_words]\n",
        "    for tweet in df['clean_text']]\n",
        "print(df['token_text'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CFM3Iwm8u3_E",
        "colab": {}
      },
      "source": [
        "#One last time, Let's look at the examples:\n",
        "print(df['text'][1])\n",
        "print(df['clean_text'][1])\n",
        "print(df['token_text'][1])\n",
        "print(\" \\n\")\n",
        "\n",
        "print(df['text'][6])\n",
        "print(df['clean_text'][6])\n",
        "print(df['token_text'][6])\n",
        "print(\" \\n\")\n",
        "\n",
        "print(df['text'][19])\n",
        "print(df['clean_text'][19])\n",
        "print(df['token_text'][19])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Z8vk0hrVUFF",
        "colab_type": "text"
      },
      "source": [
        "#### Bonus. Other cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHcdGIylXSp_",
        "colab_type": "text"
      },
      "source": [
        "##### Clean up mentions and hashtags\n",
        "We might want to explore the network of tweeps looking at who is mentioned in a tweet, but GOT3 doesn't always  extract mentions or hashtags cleanly. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQ3uL1u_VZrh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#cleans empty @s \n",
        "df['mentions'] = [re.sub(r\"(@(\\s|$))\",'', str(x)) for x in df['mentions']]\n",
        "#cleans URL artifacts in mentions\n",
        "df['mentions'] = [re.sub(r\"((?:https?(?:www)?|pic|www)(?:(?:\\s|$)))\",'', str(x)) for x in df['mentions']]\n",
        "\n",
        "#cleans empty #s \n",
        "df['hashtags'] = [re.sub(r\"(#(\\s|$))\",'', str(x)) for x in df['hashtags']]\n",
        "#cleans URL artifacts in mentions\n",
        "df['hashtags'] = [re.sub(r\"((?:https?(?:www)?|pic|www)(?:(?:\\s|$)))\",'', str(x)) for x in df['hashtags']]\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3y-V7bQdZRwr",
        "colab_type": "text"
      },
      "source": [
        "##### Tweet Length and Bots\n",
        "The longer a tweet is, the more unlikely it is that another tweep would tweet the same message word for word. Identical long tweets are more likely to be the work of Tweetbots.\n",
        "E.g. in a sample of 5000 vaccines tweets, it's likely that two individuals independently tweeted \"say no vaccines\", but a tweet like \"VALIDATE BEFORE YOU VACCINATE Giving vaccines to pets is not a risk-free procedure. It's important therefore, to weigh up the pro's and con's carefully, before deciding on the best way forward\" is too specific to be spontaneously replicated.  \n",
        " \n",
        "NB: Data does not contain retweets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYFUiTlBZXto",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Let's get an extra column with tweet length\n",
        "df['tweet_length']  = df['clean_text'].str.len() #based on length\n",
        "#And let's take out all tweets that are more than 50 characters after cleaning\n",
        "dflong = df[df.tweet_length > 50]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQmCI9p6bq99",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#How many do we have?\n",
        "len(dflong)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AHKieoydB0u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Let's separate the duplicates\n",
        "\n",
        "df.sort_values(\"clean_text\", inplace = True) \n",
        "duplicate_tweet = df[df.duplicated(['clean_text'],keep=False)]\n",
        "#how many do we have?\n",
        "len(duplicate_tweet)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-EJeTk6hsF-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#how many unique ones?\n",
        "len(duplicate_tweet.clean_text.unique())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_B-migAoWFG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#and how many times do they appear?\n",
        "duplicate_tweet['count'] = duplicate_tweet.groupby('clean_text')['clean_text'].transform('count')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUAzon-Yd3sz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Let's see what they are saying:\n",
        "\n",
        "#We start by iterating through each unique duplicate\n",
        "for n in duplicate_tweet.clean_text.unique():\n",
        "  #Here we use a bit of a trick: it's easier to read a tweet than a cleaned tweet\n",
        "  #So let's just locate the corresponding tweet for each unique cleaned tweet\n",
        "  print(duplicate_tweet.loc[duplicate_tweet['clean_text'] == n, 'text'].iloc[0],\n",
        "  #Wouldn't be nice to see the count too?\n",
        "        \" : \",\n",
        "        duplicate_tweet.loc[duplicate_tweet['clean_text'] == n, 'count'].iloc[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcGDNUoVf1g1",
        "colab_type": "text"
      },
      "source": [
        "So what do you think? Worth keeping?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ne5hmdPIf1Ec",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#If not, run this. Otherwise, leave this cell alone\n",
        "dflong.drop_duplicates(subset =\"clean_text\", keep = False, inplace = True) \n",
        "\n",
        "dfshort = df[df.tweet_length <= 50]\n",
        "df = pd.concat([dfshort, dflong], ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TcMBgYH9Q94",
        "colab_type": "text"
      },
      "source": [
        "## Discussion\n",
        "\n",
        "Can you think of more cleaning we could do? Can you justify doing it? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GvZEJ-RyrOb",
        "colab_type": "text"
      },
      "source": [
        "Hashtags?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bu471JjOynj0",
        "colab_type": "text"
      },
      "source": [
        "Spelling Correction?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NT9Q3TkQH3wm",
        "colab_type": "text"
      },
      "source": [
        "# 3. Topic modeling\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYyK0-No2Nol",
        "colab_type": "text"
      },
      "source": [
        "**Skip next two cells to proceed to topic modeling with preprocessed tweets from above.**\n",
        "\n",
        "Otherwise, load from your local drive.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ReOAcA1eGOo5",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVksv6Y_O9sF",
        "colab_type": "text"
      },
      "source": [
        "### Create, Run, and Train the HDP model via Gensim\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6L3mWQOTH3wq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets = df['token_text']\n",
        "\n",
        "dictionary = corpora.Dictionary(tweets)\n",
        "corpus = [dictionary.doc2bow(twt) for twt in tweets]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEEIGIvr1CIy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Human readable format of corpus (term-frequency)\n",
        "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKCQE1qIsEjc",
        "colab_type": "text"
      },
      "source": [
        "## Latent semantic indexing (LSI)\n",
        "This is a useful topic modeling algorithm in that it can rank topics by itself. Thus it outputs topics in a ranked order. However it does require a num_topics parameter (set to 200 by default) to determine the number of latent dimensions after the SVD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1Vc_zxvrbYD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lsimodel = LsiModel(corpus=corpus, num_topics=10, id2word=dictionary)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eo6xjOA-rbhH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lsimodel.show_topics(num_topics=5)  # Showing only the top 5 topics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1bPZux4rbnS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "lsitopics = lsimodel.show_topics(formatted=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Own7BrH6rbqX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNzmXV3zscmW",
        "colab_type": "text"
      },
      "source": [
        "### Discussion question \n",
        "\n",
        "Since all of our texts were collecting on the keyword \"vaccine\", this word occurs in almost all topic clusters. Should we remove it? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pDyX7krtNvA",
        "colab_type": "text"
      },
      "source": [
        "## Hierarchical Dirichlet process (HDP)\n",
        "An HDP model is fully unsupervised. It can also determine the ideal number of topics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh_DX6usQcqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hdp = HdpModel(corpus, id2word=dictionary)\n",
        "hdp.print_topics()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPXZy7sa3Arb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lda1 = hdp.hdp_to_lda()\n",
        "print(lda1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6FOn3PTKyPG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dictionary = corpora.Dictionary(tweets)\n",
        "corpus1 = [dictionary.doc2bow(text) for text in tweets]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iAl2lofECo6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_topics = 7\n",
        "lda = models.LdaModel(corpus1, id2word=dictionary, num_topics=total_topics)\n",
        "lda.show_topics(total_topics,10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MV9z7nQlNpXI",
        "colab_type": "text"
      },
      "source": [
        "LSA - Visualize topics through an interactive graph - pyLDAvis\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8lmuTaHH3ww",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "pyLDAvis.enable_notebook()\n",
        "panel = pyLDAvis.gensim.prepare(hdp, corpus, dictionary, mds='TSNE')\n",
        "panel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTbVUAWqEAa-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-KriKBWOMOU",
        "colab_type": "text"
      },
      "source": [
        "### Visualize cosine metrics of topics as a heatmap \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NF5kvH7XXUlA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import OrderedDict\n",
        "#set a range of topics found - in this case we set 5\n",
        "data_hdp = {i: OrderedDict(hdp.show_topic(i,20)) for i in range(6)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vc_jypKCXPfw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_hdp = pd.DataFrame(data_hdp)\n",
        "df_hdp = df_hdp.fillna(0).T\n",
        "print(df_hdp.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xJr1A2HYOS_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_hdp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGpXMRG-W4wQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "g=sns.clustermap(df_hdp.corr(), center=0, cmap=\"RdBu\", metric='cosine', linewidths=1, figsize=(10, 12))\n",
        "plt.setp(g.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3QI2QjwOjMC",
        "colab_type": "text"
      },
      "source": [
        "### HDP and LDA via Gensim Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0tD4edAPQnh",
        "colab_type": "text"
      },
      "source": [
        "We can utilize Latent Dirichlet Allocation (LDA), since the implementation in Gensim is straightforward. As before, we need to create a Dictionary and a Corpus, set the number of topics we want to infer and then finally associated a number of keywords for each topic.\n",
        "\n",
        "HDP is an implementation of LDA, but the latter lets you infer the distributions while during HDP this inference is integrated in the model without any a priori knowledge of topics\n",
        "\n",
        "In the example below, we set =5 topics and =10 keywords\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPd6OAOKH3wn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_topics = 5\n",
        "\n",
        "lda = models.LdaModel(corpus, id2word=dictionary, num_topics=total_topics)\n",
        "lda.show_topics(total_topics,10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynHlEluDZwMx",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "Heatmap of Cos Metrics for LDA\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XvRsLUtZE-O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_lda = {i: OrderedDict(lda.show_topic(i,25)) for i in range(total_topics)}\n",
        "#data_lda\n",
        "df_lda = pd.DataFrame(data_lda)\n",
        "df_lda = df_lda.fillna(0).T\n",
        "print(df_lda.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXG0D8H4Zg_k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "g=sns.clustermap(df_lda.corr(), center=0, cmap=\"RdBu\", metric='cosine', linewidths=1, figsize=(10, 12))\n",
        "plt.setp(g.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGFr9awBZyrQ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "pyLDAvis for LDA\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jx1nv7RNZlr7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pyLDAvis.gensim\n",
        "\n",
        "pyLDAvis.enable_notebook()\n",
        "panel = pyLDAvis.gensim.prepare(lda, corpus, dictionary, mds='TSNE')\n",
        "panel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tN7hLChmYNR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}